{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d03147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4eea459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3085c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Dynamic Programming, FCFS Simulated result'''\n",
    "dp = 60940\n",
    "fcfs = 45276"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dca40141",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Establish an aircraft'''\n",
    "class aircraft:\n",
    "\n",
    "    # Initialize aircraft\n",
    "    def __init__(self, adjusted_seat_price):\n",
    "        self.seat_capacity = 100\n",
    "        self.seat_type = ['Y', 'M', 'K']\n",
    "        self.adjusted_seat_price = adjusted_seat_price\n",
    "        self.init_seat_price = {'f': 0, 'Y': 800, 'M': 500, 'K': 450}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "032efff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Establish Customer class'''\n",
    "class Customer(aircraft):\n",
    "\n",
    "    # Initialize customer\n",
    "    def __init__(self):\n",
    "        super().__init__(adjusted_seat_price)\n",
    "        \n",
    "        self.customer_type = {0: 'f', 1: 'Bus1', 2:'Bus2', 3:'Leis1', 4: 'Leis2', 5: 'Leis3'}\n",
    "        self.num_customer_type = len(self.customer_type)\n",
    "        self.customer_preference = {\n",
    "            'f':{'Y': False, 'M': False, 'K': False},\n",
    "            'Bus1': {'Y': True, 'M': False, 'K': False},\n",
    "            'Bus2': {'Y': True, 'M': True, 'K': False},\n",
    "            'Leis1': {'Y': False, 'M': True, 'K': False},\n",
    "            'Leis2': {'Y': False, 'M': True, 'K': True},\n",
    "            'Leis3': {'Y': False, 'M': False, 'K': True},\n",
    "        }\n",
    "        self.consideration_set = {\n",
    "            'Bus1': {'Y': True, 'M': True, 'K': True},\n",
    "            'Bus2': {'Y': True, 'M': True, 'K': True},\n",
    "            'Leis1': {'Y': False, 'M': True, 'K': True},\n",
    "            'Leis2': {'Y': False, 'M': True, 'K': True},\n",
    "            'Leis3': {'Y': False, 'M': False, 'K': True},\n",
    "        }\n",
    "        self.wtp = {'Bus1': 1000, 'Bus2': 900, 'Leis1': 750, 'Leis2': 600, 'Leis3': 450}\n",
    "\n",
    "    '''update lambda for this RD'''\n",
    "    def update_lambda(self, init_period, new_demand, CUS_arr_percentage_dict):\n",
    "        new_lambda = new_damand/init_period\n",
    "        new_arrival_list = []\n",
    "        new_arrival_list.append(round(1-new_lambda, 2))\n",
    "        for customer in CUS_arr_percentage_dict: # arrival rates for each customer\n",
    "            new_arrival_list.append(round(CUS_arr_percentage_dict[customer] * new_lambda, 2)) \n",
    "        # print(\"new_arrival_list: \", new_arrival_list)\n",
    "        return new_arrival_list\n",
    "\n",
    "    '''customer generation'''\n",
    "    def generate_customer(self, new_arrival_list):\n",
    "        random_number = np.random.rand() # generate random number\n",
    "        probabilities = new_arrival_list # arrival rates list\n",
    "        cumulative_probability = 0 # Use cumulative probability decide customer type\n",
    "        customer_index = 0\n",
    "        for probability in probabilities:\n",
    "            cumulative_probability += probability\n",
    "            if random_number <= cumulative_probability:\n",
    "                break\n",
    "            customer_index += 1\n",
    "        customer_type = self.customer_type[customer_index] # return customer will buy what kind of seat   \n",
    "        # print(f\"random_number: {random_number}, customer_index: {customer_index}, customer_type: {customer_type}\")\n",
    "        return customer_type\n",
    "\n",
    "    '''customer's preference seat under control'''\n",
    "    def preference_seat(self, customer_type, seat_open):\n",
    "        preferred_seats = []  \n",
    "        preferences = self.customer_preference[customer_type]  \n",
    "        for seat_type, preference in preferences.items():\n",
    "            if preference and seat_type in seat_open:\n",
    "                preferred_seats.append(seat_type)\n",
    "        # print(\"preference seats: \", preferred_seats)\n",
    "        return preferred_seats  \n",
    "    \n",
    "    '''customer make decision'''\n",
    "    def make_decision(self, customer_type, seat_open):\n",
    "        preferred_seats = self.preference_seat(customer_type, seat_open)  \n",
    "        if preferred_seats:\n",
    "            cheapest_seat = min(preferred_seats, key=lambda x: self.init_seat_price[x])\n",
    "            return cheapest_seat\n",
    "        else:\n",
    "            return 'f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f61b936",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Demand model from Balaiyan et al.'''\n",
    "class demandmodel(Customer):\n",
    "    \n",
    "    # Initialize demand model\n",
    "    def __init__(self, adjusted_seat_price):\n",
    "        super().__init__(adjusted_seat_price)\n",
    "        \n",
    "        self.version = 'This is Balaiyan demand model version for consider customer consideration set, willingness to pay, and \\\n",
    "        the modified MNL.'\n",
    "        \n",
    "        # demand model parameters\n",
    "        self.total_booking = 105\n",
    "        self.market_share = 0.25\n",
    "        self.gamma = 0.08426\n",
    "        self.alpha = 0.001251\n",
    "        self.beta = {'DFARE':-0.006, 'LOT3':-0.944}\n",
    "        self.a = {'Y':{'DFARE':None, 'LOT3':1},\n",
    "                  'M':{'DFARE':None, 'LOT3':1},\n",
    "                  'K':{'DFARE':None, 'LOT3':1},\n",
    "                 }           \n",
    "        self.cus_seat_table = {}\n",
    "\n",
    "    # Calculate dm\n",
    "    def dm(self):\n",
    "        dm = self.total_booking/self.market_share\n",
    "        return dm\n",
    "\n",
    "    # Calcilate booking curve\n",
    "    def booking_curve(self, RD2, RD1):\n",
    "        booking_curve = math.exp(-self.gamma*RD2)-math.exp(-self.gamma*RD1)\n",
    "        return booking_curve\n",
    "    \n",
    "    # find pj+1\n",
    "    def find_p0_pj1_pj(self, preferred_seats):\n",
    "        prices = [self.adjusted_seat_price[seat_type] for seat_type in preferred_seats]\n",
    "        sorted_prices = sorted(prices, reverse=True)\n",
    "        if len(preferred_seats) == 1:\n",
    "            max_seat_type = max(preferred_seats, key=self.adjusted_seat_price.get)\n",
    "            p0 = pj1 = pj = self.adjusted_seat_price[max_seat_type]\n",
    "        else:\n",
    "            p0 = sorted_prices[len(sorted_prices)-1]\n",
    "            pj1 = sorted_prices[1]\n",
    "            pj = sorted_prices[0]\n",
    "        if customer == 'Leis3':\n",
    "            pj1 = 400\n",
    "        p0 = 400\n",
    "        return p0, pj1, pj\n",
    "\n",
    "    # Update fare difference for mnl\n",
    "    def recalculate_fare_difference(self):\n",
    "        new_fare_diff_avg = sum(self.adjusted_seat_price.values()) / (len(self.adjusted_seat_price)-1)\n",
    "        for seat_type in self.a:\n",
    "            self.a[seat_type]['DFARE'] = abs(round(self.adjusted_seat_price[seat_type] - new_fare_diff_avg, 2))        \n",
    "    \n",
    "    # Multinomial logit model\n",
    "    def mnl(self, seat_type, preferred_seats):\n",
    "        self.recalculate_fare_difference() # Update fare difference for mnl first\n",
    "        if seat_type in preferred_seats:\n",
    "            seat_value_dict = {}\n",
    "            for seat in preferred_seats: # Calculating this customer type's prefered seat Value function\n",
    "                seat_v = 0\n",
    "                for key, value in self.a[seat].items(): # Get the a & Beta from seat_type\n",
    "                    seat_v += self.beta[key] * value\n",
    "                seat_value_dict[seat] = seat_v\n",
    "            choose_prob = {}\n",
    "            for key, value in seat_value_dict.items():\n",
    "                choose_prob[key] = math.exp(seat_value_dict[key])/sum(math.exp(value) for value in seat_value_dict.values())\n",
    "        else:\n",
    "            choose_prob = {'Y':0, 'M':0, 'K':0}   \n",
    "        return choose_prob\n",
    "\n",
    "    # Calculate customer choice\n",
    "    def customer_choice(self, seat_type):\n",
    "        total_sum = 0\n",
    "        for key, customer in self.customer_type.items():\n",
    "            if customer != 'f':\n",
    "                preferenced_seats = self.consideration_set[customer]\n",
    "                preferred_seats = [seat for seat, preference in preferenced_seats.items() if preference]\n",
    "                choose_prob = self.mnl(seat_type, preferred_seats)\n",
    "                p0, pj1, pj = self.find_p0_pj1_pj(preferred_seats, self.adjusted_seat_price)\n",
    "                sum_of_set = (math.exp(-self.alpha*(pj-p0))-math.exp(-self.alpha*(pj1-p0))) * choose_prob[seat_type]\n",
    "                total_sum += sum_of_set\n",
    "                if customer not in self.cus_seat_table:\n",
    "                    self.cus_seat_table[customer] = {}\n",
    "                self.cus_seat_table[customer][seat_type] = sum_of_set\n",
    "        return total_sum\n",
    "\n",
    "    # Calculate demand\n",
    "    def formulation(self, RD2, RD1):\n",
    "        dm = self.dm()\n",
    "        booking_curve = self.booking_curve(RD2, RD1)\n",
    "        BR_dict = {}\n",
    "        for seat in self.seat_set:\n",
    "            customer_choice = self.customer_choice(seat)\n",
    "            BR = dm * booking_curve * customer_choice\n",
    "            BR_dict[seat] = BR\n",
    "        print(\"total demand: \", sum(BR_dict.values()))  \n",
    "        # Calculate the arrival percentage of every customer in the dict\n",
    "        CUS_arr_percentage_dict = self.calculate_customer_arrival_rate(dm, booking_curve, RD2, RD1)    \n",
    "        return BR_dict, CUS_arr_percentage_dict\n",
    "    \n",
    "    # Verified total demand from customer type, need to operate function formulation first\n",
    "    def calculate_customer_arrival_rate(self, dm, booking_curve, RD2, RD1):    \n",
    "        CUS_dict = {}\n",
    "        sum_of_cus_total_value = {} \n",
    "        \n",
    "        # Calculate every customer's total value\n",
    "        for customer, values in self.cus_seat_table.items():\n",
    "            sum_of_cus_total_value[customer] = sum(values.values())  \n",
    "            \n",
    "        # Calculate every customer's total demand\n",
    "        for customer, total_value in sum_of_cus_total_value.items():\n",
    "            BR = dm * booking_curve * total_value\n",
    "            CUS_dict[customer] = BR\n",
    "            # print(' customer: ', customer, ' predicted demand model from ', RD2,' to ', RD1, 'is', BR)\n",
    "        # print('sum_of_cus_total_value:', sum_of_cus_total_value)\n",
    "        # Customer's arrival percentage in this RD\n",
    "        total_sum = sum(CUS_dict.values())  \n",
    "        CUS_arr_percentage_dict = {key: '{:.2f}%'.format((value / total_sum) * 100) for key, value in CUS_dict.items()}\n",
    "        return CUS_arr_percentage_dict       \n",
    "    \n",
    "    # Plot demand result\n",
    "    def plot_demand(self, RD2, RD1):\n",
    "        \n",
    "        # Store calculated result\n",
    "        every_rd = {seat: [] for seat in self.seat_type}  \n",
    "        cumulative_demand = {seat: [] for seat in self.seat_type}  \n",
    "        total_demand = {seat: 0 for seat in self.seat_type}  \n",
    "        cumulative_total_demand_per_rd = []  # Store each RD cumulative demand\n",
    "\n",
    "        # Calculate all RD demand\n",
    "        cumulative_total_demand = 0  # Initialize cumulative demand\n",
    "        for i in range(RD1, RD2):\n",
    "            print('-------------------RD: ', i+1 ,'to RD: ',i, '----------------------')\n",
    "            BR_dict, CUS_dict = self.formulation(i+1, i)  \n",
    "            total_demand_rd = sum(BR_dict.values())  # Calculate demand form all rd\n",
    "            cumulative_total_demand += total_demand_rd  # update cumulative total deamnd\n",
    "            print('cumulative_total_demand: ', cumulative_total_demand)\n",
    "            cumulative_total_demand_per_rd.append(cumulative_total_demand)  # append cumulative total demand\n",
    "            for seat, demand in BR_dict.items():  \n",
    "                every_rd[seat].append(demand) \n",
    "                total_demand[seat] += demand  \n",
    "                cumulative_demand[seat].append(total_demand[seat])  \n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(RD1, RD2), cumulative_total_demand_per_rd, label='Total Demand')\n",
    "        plt.xlabel('RD')\n",
    "        plt.ylabel('Total Demand')\n",
    "        plt.title('Total Demand Model')\n",
    "        plt.legend()\n",
    "        plt.xticks(range(0, RD2, int((RD2)/10)))\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa426365",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Action space'''\n",
    "class AgentActionSpacev0:\n",
    "    def __init__(self):\n",
    "        self.action_list = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "        # self.action_list = [0, 1, 2, 3]\n",
    "        self.n = len(self.action_list)  \n",
    "\n",
    "    def sample(self):\n",
    "        return np.random.choice(self.action_list)\n",
    "\n",
    "    def contains(self, action):\n",
    "        return action in self.action_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e300a8a-793e-4b73-b515-1585f9d361a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space:  MultiDiscrete([4 4 4])\n",
      "action space.n:  64\n",
      "sample:  [3 2 0]\n",
      "contains:  True\n",
      "MultiDiscrete([2 3])\n",
      "action space:  MultiDiscrete([2 3])\n",
      "action space.n:  6\n",
      "sample:  [0 0]\n",
      "contains:  True\n"
     ]
    }
   ],
   "source": [
    "# import gym\n",
    "# from gym.spaces import MultiDiscrete\n",
    "\n",
    "# class AgentActionSpace(gym.spaces.MultiDiscrete):\n",
    "#     def __init__(self, nvec):\n",
    "#         super(AgentActionSpace, self).__init__(nvec)\n",
    "#         self.n = np.prod(nvec)\n",
    "\n",
    "#     def sample(self):\n",
    "#         return np.random.randint(0, np.array(self.nvec), dtype=int)\n",
    "\n",
    "#     def contains(self, action):\n",
    "#         if isinstance(action, list) and len(action) == len(self.nvec):\n",
    "#             for a, n in zip(action, self.nvec):\n",
    "#                 if a < 0 or a >= n:\n",
    "#                     return False\n",
    "#             return True\n",
    "#         else:\n",
    "#             return False\n",
    "\n",
    "# # Example usage\n",
    "# nvec = [4, 4, 4]  # 每个子集的动作数量\n",
    "# action_space = AgentActionSpace(nvec)\n",
    "# print('action space: ', action_space)\n",
    "# print('action space.n: ', action_space.n)\n",
    "# print('sample: ', action_space.sample())  # 随机采样一个动作\n",
    "# print('contains: ', action_space.contains([1, 2, 3]))  # 检查动作是否有效\n",
    "\n",
    "# md_space = MultiDiscrete([2, 3])\n",
    "# print(md_space)\n",
    "# print('action space: ', md_space)\n",
    "# print('action space.n: ', md_space.nvec.prod())\n",
    "# print('sample: ', md_space.sample())  # 随机采样一个动作\n",
    "# print('contains: ', md_space.contains([1, 2]))  # 检查动作是否有效"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04ff1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Establish env with Single Cabin mulitiple fare classes'''\n",
    "class AirlineEnvironment:\n",
    "    \n",
    "    '''Initialize env parameters'''\n",
    "    def __init__(self, name):\n",
    "        \n",
    "        # name of env\n",
    "        self.name = name\n",
    "        \n",
    "        # inherent aircraft\n",
    "        self.aircraft = aircraft()\n",
    "        self.seat_capacity = self.aircraft.seat_capacity # seat limitation\n",
    "        self.seat_type = self.aircraft.seat_type # seat type\n",
    "        self.init_seat_price = self.aircraft.init_seat_price\n",
    "        \n",
    "        # inherent action space class\n",
    "        # nvec = [4, 4, 4]  # every actions for a subset\n",
    "        # self.action_space = MultiDiscrete(nvec)\n",
    "        self.agent_action = AgentActionSpace() \n",
    "        self.action_space = self.agent_action.action_list\n",
    "        \n",
    "        # inherent attributes from demand model\n",
    "        self.max_rd = 20 # total selling RDs\n",
    "        self.demand_model = demandmodel()\n",
    "        self.initial_lambda = 0.8 # initial lambda for customer arrival\n",
    "\n",
    "        # inherent attributes from Customer class\n",
    "        self.customer = Customer() \n",
    "        \n",
    "        # environment parameters\n",
    "        self.seat_remain = self.seat_capacity # seat limitation\n",
    "        self.state = np.array([self.seat_remain, self.max_rd]) # Initialize state : (num seat sold, period)\n",
    "\n",
    "    '''reset env'''\n",
    "    def reset(self):\n",
    "        self.seat_remain = self.seat_capacity # Initialize total seat \n",
    "        self.state = np.array([self.seat_remain, self.max_rd])  # Initialize state\n",
    "        return self.state\n",
    "\n",
    "    # '''action correlated to the price adjust'''\n",
    "    # def action_policy(self, action):\n",
    "    #     if action\n",
    "    \n",
    "    '''Step'''\n",
    "    def step(self, state, action, price_status_dict):\n",
    "\n",
    "        # total reward this RD\n",
    "        total_reward_rd = 0\n",
    "\n",
    "        # Agent choose a price policy to update the \" price_status_dict \"\n",
    "        adjusted_policy = self.action_space[action]\n",
    "        price_status_dict = {key: round(value * adjusted_policy[i], 2) for i, (key, value) in enumerate(price_status_dict.items())}\n",
    "        print(\"price adjust: \", price_status_dict)\n",
    "\n",
    "        # Calculate periods of this RD\n",
    "        init_periods = sum(self.demand_model.formulation(state[1], state[1]-1, self.init_seat_price).values())/self.initial_lambda\n",
    "        new_demand = sum(self.demand_model.formulation(state[1], state[1]-1, price_status_dict).values())\n",
    "        new_arrival_list = update_lambda(init_period, new_demand, cus_arr_prob)\n",
    "        \n",
    "        # calculate reward this period\n",
    "        for period in range(1, init_periods):\n",
    "        \n",
    "            # With remaining seat\n",
    "            if self.seat_remain > 0:\n",
    "\n",
    "                # Customer generation\n",
    "                customer_type = self.customer.generate_customer(new_arrival_list)\n",
    "                # print(\"customer type: \", customer_type)\n",
    "    \n",
    "                # Customer choose seat\n",
    "                chosen_seat = self.customer.make_decision(customer_type, seat_open) \n",
    "                # print(\"chosen seat: \", chosen_seat)\n",
    "    \n",
    "                # Decide immediate revenue\n",
    "                reward = self.aircraft.seat_price[chosen_seat] \n",
    "    \n",
    "                # Update seat remain\n",
    "                if reward > 0:\n",
    "                    self.seat_remain = self.seat_remain-1\n",
    "            \n",
    "            # Without remaining seat \n",
    "            else:\n",
    "                # print(\"No remaining seat.\")\n",
    "                reward = 0\n",
    "\n",
    "        # Update period\n",
    "        next_time = state[1].item()-1\n",
    "        \n",
    "        # Check departure or not \n",
    "        departure = (next_time <= 0)\n",
    "\n",
    "        # update state\n",
    "        state[0] = self.seat_remain\n",
    "        state[1] = next_time\n",
    "        return state, reward, departure, price_status_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e27f316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Deep Q Network'''\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, n_actions, input_dims):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dims, 512)\n",
    "        self.fc2 = nn.Linear(512, n_actions)\n",
    "\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)\n",
    "\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        actions = self.fc2(x)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38f8bb07-026a-4a25-8942-534bd2687bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define replay buffer'''\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape),\n",
    "                                     dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape),\n",
    "                                         dtype=np.float32)\n",
    "\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool_)\n",
    "\n",
    "    # store transition to buffer\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    # sample transition from buffer\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57988436-6fdd-4f89-a63b-e997d94f28e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Define DQN agent'''\n",
    "class DQNAgent(object):\n",
    "    def __init__(self, gamma, epsilon, lr, n_actions, input_dims,\n",
    "                 mem_size, batch_size, eps_min, eps_dec, replace):\n",
    "        self.gamma = gamma # time discount gamma\n",
    "        self.epsilon = epsilon # epilson-greedy hyperparameter eplison\n",
    "        self.lr = lr # learning rate\n",
    "        self.n_actions = n_actions # number of action\n",
    "        self.input_dims = input_dims # number of state\n",
    "        self.batch_size = batch_size # batch size of sample memory\n",
    "        self.eps_min = eps_min # minimum of hyperparameter epilson\n",
    "        self.eps_dec = eps_dec # epilson decay rate, higher represent slower decay\n",
    "        self.replace_target_cnt = replace # frequence of replace target network \n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.learn_step_counter = 0\n",
    "        self.memory = ReplayBuffer(mem_size, (input_dims,), n_actions) # Replay buffer\n",
    "        self.q_eval = DeepQNetwork(self.lr, self.n_actions,\n",
    "                                    input_dims=self.input_dims) # Target network\n",
    "        self.q_next = DeepQNetwork(self.lr, self.n_actions,\n",
    "                                    input_dims=self.input_dims) # Policy network\n",
    "\n",
    "    # agent choose action\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon: # exploit: selection max value column\n",
    "            state = T.tensor([observation],dtype=T.float).to(self.q_eval.device)\n",
    "            actions = self.q_eval.forward(state)\n",
    "            action = T.argmax(actions).item()\n",
    "        else: # explore: randomly select aciton\n",
    "            action = np.random.choice(self.action_space)\n",
    "        return action\n",
    "\n",
    "    # store transition\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        self.memory.store_transition(state, action, reward, state_, done)\n",
    "\n",
    "    # sample memory\n",
    "    def sample_memory(self):\n",
    "        state, action, reward, new_state, done = \\\n",
    "                                self.memory.sample_buffer(self.batch_size)\n",
    "        states = T.tensor(state).to(self.q_eval.device)\n",
    "        rewards = T.tensor(reward).to(self.q_eval.device)\n",
    "        dones = T.tensor(done).to(self.q_eval.device)\n",
    "        actions = T.tensor(action).to(self.q_eval.device)\n",
    "        states_ = T.tensor(new_state).to(self.q_eval.device)\n",
    "        return states, actions, rewards, states_, dones\n",
    "\n",
    "    # update parameter of target network\n",
    "    def replace_target_network(self):\n",
    "        if self.learn_step_counter % self.replace_target_cnt == 0:\n",
    "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
    "\n",
    "    # decrease value of epilson\n",
    "    def decrement_epsilon(self):\n",
    "        self.epsilon = self.epsilon - self.eps_dec \\\n",
    "                           if self.epsilon > self.eps_min else self.eps_min\n",
    "\n",
    "    # optimizer model\n",
    "    def learn(self):\n",
    "\n",
    "        if self.memory.mem_cntr < self.batch_size: # check whether have enough memory\n",
    "            return\n",
    "\n",
    "        self.q_eval.optimizer.zero_grad() # diminish previous gradient\n",
    "\n",
    "        self.replace_target_network()\n",
    "\n",
    "        states, actions, rewards, states_, dones = self.sample_memory()\n",
    "        indices = np.arange(self.batch_size)\n",
    "\n",
    "        q_pred = self.q_eval.forward(states)[indices, actions]\n",
    "        q_next = self.q_next.forward(states_).max(dim=1)[0]\n",
    "\n",
    "        q_next[dones] = 0.0\n",
    "        q_target = rewards + self.gamma*q_next\n",
    "\n",
    "        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)\n",
    "        loss.backward()\n",
    "        self.q_eval.optimizer.step()\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        self.decrement_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "526bd8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action 49\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 49 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction\u001b[39m\u001b[38;5;124m\"\u001b[39m, action)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# env return observation\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m observation_, reward, departure, price_status_dict_ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(observation, action, price_status_dict)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# print(\"observation_: \", observation, \"reward: \",  reward, \"departure: \", departure)\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Update total revenue\u001b[39;00m\n\u001b[0;32m     39\u001b[0m total_revenue \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward \n",
      "Cell \u001b[1;32mIn[9], line 50\u001b[0m, in \u001b[0;36mAirlineEnvironment.step\u001b[1;34m(self, state, action, price_status_dict)\u001b[0m\n\u001b[0;32m     47\u001b[0m total_reward_rd \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Agent choose a price policy to update the \" price_status_dict \"\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m adjusted_policy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space[action]\n\u001b[0;32m     51\u001b[0m price_status_dict \u001b[38;5;241m=\u001b[39m {key: \u001b[38;5;28mround\u001b[39m(value \u001b[38;5;241m*\u001b[39m adjusted_policy[i], \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i, (key, value) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(price_status_dict\u001b[38;5;241m.\u001b[39mitems())}\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice adjust: \u001b[39m\u001b[38;5;124m\"\u001b[39m, price_status_dict)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\RL_env\\Lib\\site-packages\\gym\\spaces\\multi_discrete.py:154\u001b[0m, in \u001b[0;36mMultiDiscrete.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m    153\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Extract a subspace from this ``MultiDiscrete`` space.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 154\u001b[0m     nvec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnvec[index]\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nvec\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    156\u001b[0m         subspace \u001b[38;5;241m=\u001b[39m Discrete(nvec)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 49 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "# Initialize Training parameters\n",
    "num_episodes = 10\n",
    "DQN_total_revenues = []\n",
    "DQN_cumulative_average = []\n",
    "DQN_per_50_avg = []\n",
    "\n",
    "env = AirlineEnvironment(\"Single route parallel flight.v0\")\n",
    "agent = DQNAgent(gamma=0.95, epsilon=1, lr=0.0001,\n",
    "                 input_dims=len(env.state),\n",
    "                 n_actions=env.action_space.nvec.prod(), mem_size=300000, eps_min=0.1,\n",
    "                 batch_size=256, replace=1000, eps_dec=1e-5)\n",
    "\n",
    "n_step = 0\n",
    "best_score = 0\n",
    "\n",
    "# Start Training\n",
    "for episode in range(1, num_episodes+1):\n",
    "    \n",
    "    # Initialize environment\n",
    "    # print(\"---------- Episode \", episode, \"-----------\")\n",
    "    observation = env.reset()\n",
    "    total_revenue = 0 # Initialize total revenue\n",
    "    departure = None\n",
    "    price_status_dict = env.init_seat_price\n",
    "    \n",
    "    while not departure:\n",
    "        \n",
    "        # print(\"------- step \" , env.max_rd - n_step,\" ---------\")\n",
    "        \n",
    "        # agent select action\n",
    "        action = agent.choose_action(observation)\n",
    "        print(\"action\", action)\n",
    "\n",
    "        # env return observation\n",
    "        observation_, reward, departure, price_status_dict_ = env.step(observation, action, price_status_dict)\n",
    "        # print(\"observation_: \", observation, \"reward: \",  reward, \"departure: \", departure)\n",
    "        \n",
    "        # Update total revenue\n",
    "        total_revenue += reward \n",
    "\n",
    "        # agent memorize transition\n",
    "        agent.store_transition(observation, action, reward, observation_, departure)\n",
    "        agent.learn()\n",
    "\n",
    "        # Move on to the next state\n",
    "        observation = observation_\n",
    "\n",
    "        # Move on to the next price point\n",
    "        price_status_dict = price_status_dict_\n",
    "        n_step += 1\n",
    "\n",
    "    if total_revenue > best_score:\n",
    "        best_score = total_revenue\n",
    "        \n",
    "    DQN_total_revenues.append(total_revenue) \n",
    "    DQN_per_50_avg.append(np.mean(DQN_total_revenues[-50:])) \n",
    "    DQN_cumulative_avg = sum(DQN_total_revenues) / (episode)\n",
    "    DQN_cumulative_average.append(DQN_cumulative_avg)\n",
    "    print('Episode:',episode ,'Total Revenue:', total_revenue,\n",
    "             'Cumulative average %.1f' % DQN_cumulative_avg, 'best score:', best_score,\n",
    "            'epsilon %.2f' % agent.epsilon, 'steps', n_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b61abb-420c-43c5-a67a-8da494fe849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.plot(range(num_episodes), DQN_total_revenues, label='DQN each episode', alpha=0.5, color='b')\n",
    "plt.plot(range(num_episodes), DQN_per_50_avg, label='DQN per 50 episode', alpha=0.5, color='green')\n",
    "plt.plot(range(num_episodes), DQN_cumulative_average, label='DQN Cumulative')\n",
    "plt.axhline(y=dp, color='r', label='DP')\n",
    "plt.axhline(y=fcfs, color='orange', label='FCFS')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Reward per Episode')\n",
    "plt.legend()  \n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
