{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eea459",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3085c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Dynamic Programming, FCFS Simulated result'''\n",
    "dp = 60940\n",
    "fcfs = 45276"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca40141",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Establish an aircraft'''\n",
    "class aircraft:\n",
    "\n",
    "    # Initialize aircraft\n",
    "    def __init__(self):\n",
    "        self.seat_capacity = 100\n",
    "        self.seat_type = ['Y', 'M', 'K']\n",
    "        self.seat_price = {'f': 0, 'Y': 800, 'M': 500, 'K': 450}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f61b936",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Demand model from Balaiyan et al.'''\n",
    "class demandmodel:\n",
    "    \n",
    "    # Initialize demand model\n",
    "    def __init__(self):\n",
    "        \n",
    "        # inherent attributes from aircraft class\n",
    "        self.aircraft = aircraft()\n",
    "        self.seat_set = self.aircraft.seat_type\n",
    "        self.seat_price = self.aircraft.seat_price\n",
    "        \n",
    "        # demand model parameters\n",
    "        self.total_booking = 105\n",
    "        self.market_share = 0.25\n",
    "        self.gamma = 0.08426\n",
    "        self.alpha = 0.001251\n",
    "        self.beta = {'DFARE':-0.006, 'LOT3':-0.944}\n",
    "        self.a = {'Y':{'DFARE':800, 'LOT3':1},\n",
    "                  'M':{'DFARE':500, 'LOT3':1},\n",
    "                  'K':{'DFARE':450, 'LOT3':1},\n",
    "                 }\n",
    "        self.fare_diff_avg = sum(self.seat_price.values()) / len(self.seat_price)\n",
    "        for seat_type in self.a:\n",
    "            self.a[seat_type]['DFARE'] = round(self.a[seat_type]['DFARE'] - self.fare_diff_avg, 2)\n",
    "\n",
    "    # Calculate dm\n",
    "    def dm(self):\n",
    "        dm = self.total_booking/self.market_share\n",
    "        return dm\n",
    "\n",
    "    # Calcilate booking curve\n",
    "    def booking_curve(self, RD2, RD1):\n",
    "        booking_curve = math.exp(-self.gamma*RD2)-math.exp(-self.gamma*RD1)\n",
    "        return booking_curve\n",
    "\n",
    "    # find pj+1\n",
    "    def find_pj1(self, seat):\n",
    "        smaller_keys = [key for key in self.seat_price.keys() if key < seat]\n",
    "        if smaller_keys:\n",
    "            max_smaller_key = max(smaller_keys)\n",
    "            pj1 = self.seat_price[max_smaller_key]\n",
    "        else:\n",
    "            pj1 = min(self.seat_price.values())\n",
    "        return pj1\n",
    "\n",
    "    # Multinomial logit model\n",
    "    def mnl(self):\n",
    "        choose_prob = {}\n",
    "        value_dict = {}\n",
    "        for seat in self.seat_set:\n",
    "            result = {key: self.beta[key] * value for key, value in self.a[seat].items()}\n",
    "            total = sum(result.values())\n",
    "            value = math.exp(total)\n",
    "            value_dict[seat] = value\n",
    "        for seat in value_dict:\n",
    "            exp_value = math.exp(value_dict[seat])\n",
    "            choose_prob[seat] = exp_value / sum(math.exp(value) for value in value_dict.values())\n",
    "        # print(\"mnl: \", choose_prob)\n",
    "        return choose_prob\n",
    "\n",
    "    # Calculate customer choice\n",
    "    def customer_choice(self, seat_type):\n",
    "        total_sum = 0\n",
    "        choose_prob = self.mnl()\n",
    "        min_key = min(self.seat_price, key=self.seat_price.get)\n",
    "        p0 = self.seat_price[min_key]\n",
    "        for seat in self.seat_set:\n",
    "            pj = self.seat_price['Y'] # 暫時寫這樣\n",
    "            pj1 = self.seat_price['M'] # 暫時寫這樣\n",
    "            # print('p0: ',p0 , 'p1: ', pj, 'pj1: ', pj1)\n",
    "            sum_of_set = (math.exp(-self.alpha*(pj-p0))-math.exp(-self.alpha*(pj1-p0))) * choose_prob[seat_type] \n",
    "            # print('sum of set: ', sum_of_set)\n",
    "            total_sum += sum_of_set\n",
    "        return total_sum\n",
    "\n",
    "    # Calculate demand\n",
    "    def formulation(self, RD2, RD1):\n",
    "        dm = self.dm()\n",
    "        # print('dm',dm)\n",
    "        booking_curve = self.booking_curve(RD2, RD1)\n",
    "        # print('booking curve: ',booking_curve)\n",
    "        BR_dict = {}\n",
    "        \n",
    "        for seat in self.seat_set:\n",
    "            # print(\"calculate\", seat, \" ing...\")\n",
    "            customer_choice = self.customer_choice(seat)\n",
    "            # print('customer choice', customer_choice)\n",
    "            BR = dm * booking_curve * customer_choice\n",
    "            BR_dict[seat] = BR\n",
    "            # print(' seat: ', seat, ' predicted demand model from ', RD2,' to ', RD1, 'is', BR)\n",
    "        # print(\"total demand: \", sum(BR_dict.values()))\n",
    "        return BR_dict\n",
    "    \n",
    "    # Plot demand result\n",
    "    def plot_demand(self):\n",
    "        \n",
    "        # Store calculated result\n",
    "        every_rd = {seat: [] for seat in model.seat_set}  \n",
    "        cumulative_demand = {seat: [] for seat in model.seat_set}  \n",
    "        total_demand = {seat: 0 for seat in model.seat_set}  \n",
    "        cumulative_total_demand_per_rd = []  # Store each RD cumulative demand\n",
    "\n",
    "        # Calculate all RD demand\n",
    "        cumulative_total_demand = 0  # Initialize cumulative demand\n",
    "        for i in range(2, max_rd+1):\n",
    "            # print('i: ', i)\n",
    "            BR_results = model.formulation(i, i-1)  \n",
    "            total_demand_rd = sum(BR_results.values())  # Calculate demand form all rd\n",
    "            cumulative_total_demand += total_demand_rd  # update cumulative total deamnd\n",
    "            cumulative_total_demand_per_rd.append(cumulative_total_demand)  # append cumulative total demand\n",
    "            for seat, demand in BR_results.items():  \n",
    "                every_rd[seat].append(demand) \n",
    "                total_demand[seat] += demand  \n",
    "                cumulative_demand[seat].append(total_demand[seat])  \n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, max_rd), cumulative_total_demand_per_rd, label='Total Demand')\n",
    "        plt.xlabel('RD')\n",
    "        plt.ylabel('Total Demand')\n",
    "        plt.title('Total Demand Model')\n",
    "        plt.legend()\n",
    "        plt.xticks(range(0, max_rd, int(max_rd/10)))\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa426365",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Action space'''\n",
    "class AgentActionSpace:\n",
    "    def __init__(self):\n",
    "        # self.action_list = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "        self.action_list = [0, 1, 2, 3]\n",
    "        self.n = len(self.action_list)  \n",
    "\n",
    "    def sample(self):\n",
    "        return np.random.choice(self.action_list)\n",
    "\n",
    "    def contains(self, action):\n",
    "        return action in self.action_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784db100",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Uniform distribution'''\n",
    "class uniform_distribution:\n",
    "\n",
    "    # Initialize parameters\n",
    "    def __init__(self, total_demand, total_lambda):\n",
    "        self.total_demand = total_demand\n",
    "        self.total_lambda = total_lambda\n",
    "        self.num_period = int(total_demand/total_lambda)\n",
    "        self.prob = {'Bus1': 0.1, 'Bus2': 0.2, 'Leis1': 0.2, 'Leis2': 0.2, 'Leis3': 0.3}\n",
    "    \n",
    "    # Calculate lambda for each type of customer\n",
    "    def calculate_lambda(self):\n",
    "        lambda_list = []\n",
    "        lambda_list.append(round(1-self.total_lambda, 2))\n",
    "        for customer in self.prob:\n",
    "            lambda_list.append(round(self.prob[customer] * self.total_lambda, 2)) \n",
    "        # print(\"lambda_list: \", lambda_list)\n",
    "        return lambda_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032efff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Establish Customer class'''\n",
    "class Customer:\n",
    "\n",
    "    ''' Initialization '''\n",
    "    def __init__(self, total_demand, total_lambda):\n",
    "        self.customer_type = {0: 'f', 1: 'Bus1', 2:'Bus2', 3:'Leis1', 4: 'Leis2', 5: 'Leis3'}\n",
    "        self.num_customer_type = len(self.customer_type)\n",
    "        self.customer_preference = {\n",
    "            'f':{'Y': False, 'M': False, 'K': False},\n",
    "            'Bus1': {'Y': True, 'M': False, 'K': False},\n",
    "            'Bus2': {'Y': True, 'M': True, 'K': False},\n",
    "            'Leis1': {'Y': False, 'M': True, 'K': False},\n",
    "            'Leis2': {'Y': False, 'M': True, 'K': True},\n",
    "            'Leis3': {'Y': False, 'M': False, 'K': True},\n",
    "        }\n",
    "\n",
    "        # inherent from uniform distribution class\n",
    "        self.uniform_distribution = uniform_distribution(total_demand, total_lambda)\n",
    "        self.arrival_rates = self.uniform_distribution.calculate_lambda() # calculate arrival rates for each customer type\n",
    "        \n",
    "        # inherent from aircraft class\n",
    "        self.aircraft = aircraft()\n",
    "        self.seat_price = self.aircraft.seat_price\n",
    "\n",
    "    '''Customer generation'''\n",
    "    def generate_customer(self):\n",
    "        random_number = np.random.rand() # generate random number\n",
    "        probabilities = self.arrival_rates # arrival rates list\n",
    "        cumulative_probability = 0 # Use cumulative probability decide customer type\n",
    "        customer_index = 0\n",
    "        for probability in probabilities:\n",
    "            cumulative_probability += probability\n",
    "            if random_number <= cumulative_probability:\n",
    "                break\n",
    "            customer_index += 1\n",
    "        customer_type = self.customer_type[customer_index] # return customer will buy what kind of seat   \n",
    "        # print(f\"random_number: {random_number}, customer_index: {customer_index}, customer_type: {customer_type}\")\n",
    "        return customer_type\n",
    "\n",
    "    '''customer's preference seat under control'''\n",
    "    def preference_seat(self, customer_type, seat_open):\n",
    "        preferred_seats = []  \n",
    "        preferences = self.customer_preference[customer_type]  \n",
    "        for seat_type, preference in preferences.items():\n",
    "            if preference and seat_type in seat_open:\n",
    "                preferred_seats.append(seat_type)\n",
    "        # print(\"preference seats: \", preferred_seats)\n",
    "        return preferred_seats  \n",
    "    \n",
    "    '''customer make decision'''\n",
    "    def make_decision(self, customer_type, seat_open):\n",
    "        preferred_seats = self.preference_seat(customer_type, seat_open)  \n",
    "        if preferred_seats:\n",
    "            cheapest_seat = min(preferred_seats, key=lambda x: self.seat_price[x])\n",
    "            return cheapest_seat\n",
    "        else:\n",
    "            return 'f'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Establish env with Single Cabin mulitiple fare classes'''\n",
    "class AirlineEnvironment:\n",
    "    \n",
    "    '''Initialize env parameters'''\n",
    "    def __init__(self, name):\n",
    "        \n",
    "        # name of env\n",
    "        self.name = name\n",
    "        \n",
    "        # inherent aircraft\n",
    "        self.aircraft = aircraft()\n",
    "        self.seat_capacity = self.aircraft.seat_capacity # seat limitation\n",
    "        self.seat_type = self.aircraft.seat_type # seat type\n",
    "        \n",
    "        # inherent action space class\n",
    "        self.action_space = AgentActionSpace() \n",
    "        \n",
    "        # inherent attributes from demand model\n",
    "        self.max_rd = 20 # total selling RDs\n",
    "        self.demand_model = demandmodel()\n",
    "        self.total_demand = sum(self.demand_model.formulation(self.max_rd, 1).values()) # total demand from demand model\n",
    "        self.total_lambda = 0.8 # total lambda from test\n",
    "        self.max_period = int(self.total_demand/self.total_lambda) # total period\n",
    "        \n",
    "        # inherent attributes from uniform distribution class\n",
    "        self.uniform_distribution = uniform_distribution(self.total_demand, self.total_lambda)\n",
    "        self.arrival_rates = self.uniform_distribution.calculate_lambda() # calculate arrival rates for each customer type\n",
    "\n",
    "        # inherent attributes from Customer class\n",
    "        self.customer = Customer(self.total_demand, self.total_lambda) \n",
    "        \n",
    "        # environment parameters\n",
    "        self.seat_remain = self.seat_capacity # seat limitation\n",
    "        self.state = np.array([self.seat_capacity, self.max_period]) # Initialize state : (num seat sold, period)\n",
    "        # self.a_s_ref = {0: 'f', 1: 'Y', 2: 'M', 3: 'K', 4: 'YM', 5: 'YK', 6: 'MK', 7: 'YMK'} # action reference to seat open combination\n",
    "        self.a_s_ref = {0: 'f', 1: 'Y', 2: 'YK', 3: 'YMK'}\n",
    "        \n",
    "    '''reset env'''\n",
    "    def reset(self):\n",
    "        self.seat_remain = self.seat_capacity # Initialize total seat \n",
    "        self.state = np.array([self.seat_remain, self.max_period])  # Initialize state\n",
    "        return self.state\n",
    "    \n",
    "    '''Step'''\n",
    "    def step(self, state, action):\n",
    "\n",
    "        # print(f\"Action: {action}, state: {state}\")\n",
    "        \n",
    "        # With remaining seat\n",
    "        if self.seat_remain > 0:\n",
    "\n",
    "            # Agent choose a seat combination\n",
    "            seat_open = self.a_s_ref[action]   \n",
    "            # print(\"seat_open: \", seat_open)\n",
    "            \n",
    "            # Customer generation\n",
    "            customer_type = self.customer.generate_customer()\n",
    "            # print(\"customer type: \", customer_type)\n",
    "\n",
    "            # Customer choose seat\n",
    "            chosen_seat = self.customer.make_decision(customer_type, seat_open) \n",
    "            # print(\"chosen seat: \", chosen_seat)\n",
    "\n",
    "            # Decide immediate revenue\n",
    "            reward = self.aircraft.seat_price[chosen_seat] \n",
    "\n",
    "            # Update seat remain\n",
    "            if reward > 0:\n",
    "                self.seat_remain = self.seat_remain-1\n",
    "        \n",
    "        # Without remaining seat \n",
    "        else:\n",
    "            # print(\"No remaining seat.\")\n",
    "            reward = 0\n",
    "\n",
    "        # Update period\n",
    "        next_time = state[1].item() - 1\n",
    "        \n",
    "        # Check departure or not \n",
    "        departure = (next_time <= 0)\n",
    "\n",
    "        # update state\n",
    "        state[0] = self.seat_remain\n",
    "        state[1] = next_time\n",
    "        return state, reward, departure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f316e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Deep Q Network'''\n",
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, lr, n_actions, input_dims):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dims, 512)\n",
    "        self.fc2 = nn.Linear(512, n_actions)\n",
    "\n",
    "        self.optimizer = optim.RMSprop(self.parameters(), lr=lr)\n",
    "\n",
    "        self.loss = nn.MSELoss()\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        actions = self.fc2(x)\n",
    "\n",
    "        return actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f8bb07-026a-4a25-8942-534bd2687bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Define replay buffer'''\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape),\n",
    "                                     dtype=np.float32)\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape),\n",
    "                                         dtype=np.float32)\n",
    "\n",
    "        self.action_memory = np.zeros(self.mem_size, dtype=np.int64)\n",
    "        self.reward_memory = np.zeros(self.mem_size, dtype=np.float32)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool_)\n",
    "\n",
    "    # store transition to buffer\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.terminal_memory[index] = done\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    # sample transition from buffer\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "        batch = np.random.choice(max_mem, batch_size, replace=False)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        terminal = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57988436-6fdd-4f89-a63b-e997d94f28e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''DDQN Agent'''\n",
    "class DDQNAgent(object):\n",
    "    def __init__(self, gamma, epsilon, lr, n_actions, input_dims,\n",
    "                 mem_size, batch_size, eps_min, eps_dec, replace):\n",
    "        self.gamma = gamma # time discount gamma\n",
    "        self.epsilon = epsilon # epilson-greedy hyperparameter eplison\n",
    "        self.lr = lr # learning rate\n",
    "        self.n_actions = n_actions # number of action\n",
    "        self.input_dims = input_dims # number of state\n",
    "        self.batch_size = batch_size # batch size of sample memory\n",
    "        self.eps_min = eps_min # minimum of hyperparameter epilson\n",
    "        self.eps_dec = eps_dec # epilson decay rate, higher represent slower decay\n",
    "        self.replace_target_cnt = replace # frequence of replace target network \n",
    "        self.action_space = [i for i in range(n_actions)]\n",
    "        self.learn_step_counter = 0\n",
    "        self.memory = ReplayBuffer(mem_size, (input_dims,), n_actions) # Replay buffer\n",
    "        self.q_eval = DeepQNetwork(self.lr, self.n_actions,\n",
    "                                    input_dims=self.input_dims) # Target network\n",
    "        self.q_next = DeepQNetwork(self.lr, self.n_actions,\n",
    "                                    input_dims=self.input_dims) # Policy network\n",
    "\n",
    "    # store transition\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        self.memory.store_transition(state, action, reward, state_, done)\n",
    "\n",
    "    # sample memory\n",
    "    def sample_memory(self):\n",
    "        state, action, reward, new_state, done = \\\n",
    "                                self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        states = T.tensor(state).to(self.q_eval.device)\n",
    "        rewards = T.tensor(reward).to(self.q_eval.device)\n",
    "        dones = T.tensor(done).to(self.q_eval.device)\n",
    "        actions = T.tensor(action).to(self.q_eval.device)\n",
    "        states_ = T.tensor(new_state).to(self.q_eval.device)\n",
    "\n",
    "        return states, actions, rewards, states_, dones\n",
    "\n",
    "    # agent choose action\n",
    "    def choose_action(self, observation):\n",
    "        if np.random.random() > self.epsilon:\n",
    "            state = T.tensor([observation],dtype=T.float).to(self.q_eval.device)\n",
    "            actions = self.q_eval.forward(state)\n",
    "            action = T.argmax(actions).item()\n",
    "        else:\n",
    "            action = np.random.choice(self.action_space)\n",
    "\n",
    "        return action\n",
    "\n",
    "    # update parameter of target network\n",
    "    def replace_target_network(self):\n",
    "        if self.replace_target_cnt is not None and \\\n",
    "           self.learn_step_counter % self.replace_target_cnt == 0:\n",
    "            self.q_next.load_state_dict(self.q_eval.state_dict())\n",
    "\n",
    "    # decrease value of epilson\n",
    "    def decrement_epsilon(self):\n",
    "        self.epsilon = self.epsilon - self.eps_dec \\\n",
    "                           if self.epsilon > self.eps_min else self.eps_min\n",
    "\n",
    "    # optimizer model\n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size: # check whether have enough memory\n",
    "            return\n",
    "\n",
    "        self.q_eval.optimizer.zero_grad() # diminish previous gradient\n",
    "\n",
    "        self.replace_target_network()\n",
    "\n",
    "        states, actions, rewards, states_, dones = self.sample_memory()\n",
    "\n",
    "        indices = np.arange(self.batch_size)\n",
    "\n",
    "        q_pred = self.q_eval.forward(states)[indices, actions]\n",
    "        q_next = self.q_next.forward(states_)\n",
    "        q_eval = self.q_eval.forward(states_)\n",
    "\n",
    "        max_actions = T.argmax(q_eval, dim=1)\n",
    "        q_next[dones] = 0.0\n",
    "\n",
    "        q_target = rewards + self.gamma*q_next[indices, max_actions]\n",
    "        loss = self.q_eval.loss(q_target, q_pred).to(self.q_eval.device)\n",
    "        loss.backward()\n",
    "\n",
    "        self.q_eval.optimizer.step()\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        self.decrement_epsilon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526bd8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Training parameters\n",
    "num_episodes = 3000\n",
    "DDQN_total_revenues = []\n",
    "DDQN_cumulative_average = []\n",
    "DDQN_per_50_avg = []\n",
    "\n",
    "env = AirlineEnvironment(\"Single route parallel flight.v0\")\n",
    "agent = DDQNAgent(gamma=0.99, epsilon=1, lr=0.0001,\n",
    "                 input_dims=len(env.state),\n",
    "                 n_actions=env.action_space.n, mem_size=100000, eps_min=0.1,\n",
    "                 batch_size=256, replace=1000, eps_dec=1e-5)\n",
    "\n",
    "n_step = 0\n",
    "best_score = 0\n",
    "\n",
    "# Start Training\n",
    "for episode in range(1, num_episodes+1):\n",
    "    \n",
    "    # Initialize environment\n",
    "    # print(\"---------- Episode \", episode, \"-----------\")\n",
    "    observation = env.reset()\n",
    "    total_revenue = 0 # Initialize total revenue\n",
    "    departure = None\n",
    "    \n",
    "    while not departure:\n",
    "        \n",
    "        # print(\"------- step \" , env.max_period - n_step,\" ---------\")\n",
    "        \n",
    "        # agent select action\n",
    "        action = agent.choose_action(observation)\n",
    "        # print(\"action\", action)\n",
    "\n",
    "        # env return observation\n",
    "        observation_, reward, departure = env.step(observation, action)\n",
    "        # print(\"observation_: \", observation, \"reward: \",  reward, \"departure: \", departure)\n",
    "        total_revenue += reward # Update total revenue\n",
    "\n",
    "        # agent memorize transition\n",
    "        agent.store_transition(observation, action, reward, observation_, departure)\n",
    "        agent.learn()\n",
    "\n",
    "        # Move on to the next state\n",
    "        observation = observation_ \n",
    "        n_step += 1\n",
    "\n",
    "    if total_revenue > best_score:\n",
    "        best_score = total_revenue\n",
    "        \n",
    "    DDQN_total_revenues.append(total_revenue) \n",
    "    DDQN_per_50_avg.append(np.mean(DDQN_total_revenues[-50:])) \n",
    "    DDQN_cumulative_avg = sum(DDQN_total_revenues) / (episode)\n",
    "    DDQN_cumulative_average.append(DDQN_cumulative_avg)\n",
    "    print('Episode:',episode ,'Total Revenue:', total_revenue,\n",
    "             'Cumulative average %.1f' % DDQN_cumulative_avg, 'best score:', best_score,\n",
    "            'epsilon %.2f' % agent.epsilon, 'steps', n_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff5b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))  \n",
    "plt.plot(range(num_episodes), DDQN_total_revenues, label='DDQN each episode', alpha=0.5, color='b')\n",
    "plt.plot(range(num_episodes), DDQN_per_50_avg, label='DDQN per 50 episode', alpha=0.5, color='green')\n",
    "plt.plot(range(num_episodes), DDQN_cumulative_average, label='DDQN Cumulative')\n",
    "plt.axhline(y=dp, color='r', label='DP')\n",
    "plt.axhline(y=fcfs, color='orange', label='FCFS')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('Total Reward per Episode')\n",
    "plt.legend()  \n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
